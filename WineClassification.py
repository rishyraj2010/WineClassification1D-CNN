# -*- coding: utf-8 -*-
"""ProjectML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ir8mUCobTIzk_3lNKki6KX32WWunrNsH

# White wine classification with various models

Install PyCaret and all the libraries to be used
"""

!pip install pycaret[full]

import numpy as np
import pandas as pd
from zipfile import ZipFile
import matplotlib.pyplot as plt
import seaborn as sns

# check installed version
from pycaret.utils import version
version()

"""Use the White Wine Dataset from the file"""

# loading sample dataset from pycaret dataset module

zip_file = ZipFile('wine+quality.zip')
data = pd.read_csv(zip_file.open('winequality-white.csv'), sep=';')
data.head()
data.describe()

""" Perform Elementary EDA with our target as ***Quality***"""

plt.figure(figsize=(6,4))
sns.heatmap(data.corr(),cmap='Blues',annot=False)

data['quality'].value_counts()

plt.scatter(data['quality'], data['sulphates'], color = 'red')
plt.title('Relation of sulphates with wine quality')
plt.xlabel('quality')
plt.ylabel('sulphates')
plt.legend()
plt.show()

plt.bar(data['quality'], data['alcohol'], color = 'green')
plt.title('relation of alcohol with wine quality')
plt.xlabel('quality')
plt.ylabel('alcohol')
plt.legend()
plt.show()

"""## Setup for PyCaret
It is used to create a transformation pipeline. Setup function must be called before executing any other function in PyCaret. It is used to initiate the input and target variables
"""

# import pycaret classification and init setup
from pycaret.classification import *
s = setup(data, target = 'quality', session_id = 123)

"""## Compare PyCaret Models

We will compare most of the Models available in PyCaret, few models are ommited due to their high computational requirements. If you wish to include all the models use the turbo = False line.
"""

# check available models
models()

# compare models
best = compare_models(include = ['lr','knn','nb','dt','svm','mlp','ridge','rf','qda','ada','gbc','lda','et','xgboost','lightgbm','catboost','dummy'])
#best = compare_models(turbo=False)

""" Model Analysis"""

# plot confusion matrix
plot_model(best, plot = 'confusion_matrix')
data.quality.unique()

# plot AUC
plot_model(best, plot = 'auc')

# plot feature importance
plot_model(best, plot = 'feature')

evaluate_model(best)

"""## Prediction
The `predict_model` outputs \
`prediction_label`: Quality \
`prediction_score`: Probability of the predicted quality
"""

# predict on test set
holdout_pred = predict_model(best)

# show predictions df
holdout_pred.head()

"""The same function works for predicting the labels on unseen dataset. Let's create a copy of original data and drop the `Class variable`. We can then use the new data frame without labels for scoring."""

# copy data and drop Class variable

new_data = data.copy()
new_data.drop('quality', axis=1, inplace=True)
new_data.head()

# predict model on new_data
predictions = predict_model(best, data = new_data)
predictions.head()

"""Consider the top 3 models"""

best_F1_models_top3 = compare_models(sort = 'F1', n_select = 3)

# list of top 3 models by Recall
best_F1_models_top3

"""##  Tuning the Model"""

# train a rf model with default params
rf = create_model('rf')

# tune hyperparameters of rf
tuned_rf = tune_model(rf)

tuned_rf

# tune rf using optuna
!pip install optuna
tune_model(rf, search_library = 'optuna')

"""# Blend Models"""

# top 3 models based on recall
best_F1_models_top3

# blend top 3 models
blend_models(best_F1_models_top3)

"""##  Plot Model

This function analyzes the performance of a trained model for the test(holdout set).
"""

# plot class report
plot_model(best, plot = 'class_report')

"""##  Interpret Model

This function analyzes the predictions generated from a trained model. Most plots in this function are implemented based on the SHAP (Shapley Additive exPlanations). For more info on this, please see https://shap.readthedocs.io/en/latest/
"""

# interpret summary model
interpret_model(rf, plot = 'summary')

# reason plot for test set observation 1
interpret_model(rf, plot = 'reason', observation = 1)

"""###  Finalize Model
This function trains a given model on the entire dataset including the hold-out set.
"""

final_best = finalize_model(best)

final_best

"""# Save  Model
We will save the model as PyCaret Model as we will be performing 1D-CNN for the next part
"""

# save pipeline
save_model(final_best, 'white_wine_pipeline_pycaret')

"""# Create Data for 1D-CNN

We will be creating a 1D-CNN based on reserach presented by
Shengnan Di, et al. in  '*Di, S. (2022). Prediction of red wine quality using one-dimensional convolutional neural networks.Â arXiv preprint arXiv:2208.14008.'*

Create Dataset with Correlated features next to each other
"""

# Set a threshold for correlation strength (adjust as needed)
threshold = 0.1

correlation_matrix = data.corr()

# Find highly correlated features
highly_correlated_pairs = [
    (feature1, feature2, correlation_matrix.loc[feature1, feature2])
    for feature1 in correlation_matrix.columns
    for feature2 in correlation_matrix.columns
    if feature1 != feature2 and abs(correlation_matrix.loc[feature1, feature2]) > threshold
]

# Sort the pairs by absolute correlation coefficient (descending order)
sorted_correlations = sorted(highly_correlated_pairs, key=lambda x: x[2], reverse=True)

# Select the top correlated features
top_correlations = sorted_correlations[:]

# Display the top 10 correlated features
print("Top Correlated Features:")

j = len(top_correlations)/2

sorted_feature = []

for i in range(int(j)):
  feature1, feature2, correlation = top_correlations[2*i]
  sorted_feature.append(feature1)
  sorted_feature.append(feature2)
  print(f"{feature1} and {feature2}: {correlation:.2f}")

# Create a heatmap for visualization
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()

"""Sort list of features as per the order of the features was rearranged based on the results of Pearson Correlation Analysis"""

# Get column names as an array
column_names_array = data.columns.to_numpy()
sorted_feature = np.array(sorted_feature)
#print(column_names_array)
#print(sorted_feature)

# Use numpy.unique with return_index and return_inverse to get unique elements
unique_array, indices = np.unique(sorted_feature, return_index=True)

# Print the result
#print(indices)

sort_indices = np.sort(indices)

feature_list = []

for k in range(len(unique_array)):
  feature_list.append(sorted_feature[sort_indices[k]])

print(feature_list)


CNN_df = data[feature_list]

CNN_df.head()

# dividing the dataset into input and target variables

y = CNN_df['quality']
CNN_df.pop('quality')
x = CNN_df

# determining the shape of x and y.
print(x)
print(y)

"""Modelling 1D-CNN"""

#!pip install tensorflow

# standard scaling
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
x = sc.fit_transform(x)

print(x)

import tensorflow as tf

# Example class vector (replace this with your actual labels)
y_new = np.array(y)
x_new = np.array(x)
#print(y_new)
#print(x_new)
one_hot_encoded_y = tf.keras.utils.to_categorical(y_new, num_classes=10)

x_new = x_new.reshape(x_new.shape[0], x_new.shape[1],1)
#one_hot_encoded_y = one_hot_encoded_y.reshape(one_hot_encoded_y.shape[0],1, one_hot_encoded_y.shape[1])
print(one_hot_encoded_y.shape)
print(x_new.shape)
print(one_hot_encoded_y)

from sklearn.model_selection import train_test_split
# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x_new, one_hot_encoded_y, test_size=0.3, random_state=42)

x_train[0]

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models


# Generate some example data (replace this with your own data)
num_samples = x_train.shape[0]
num_features = x_train.shape[1]
sequence_length = x_train.shape[2]

# Define the 1D-CNN model
model = models.Sequential()

# Add a 1D convolutional layer with 32 filters and a kernel size of 3
model.add(layers.Conv1D(32, kernel_size = 3, activation = 'relu', input_shape=(num_features, sequence_length)))

# Flatten the output to feed it into densely connected layers
model.add(layers.Flatten())

# Add a dense layer with 64 units
model.add(layers.Dense(1000, activation='relu'))

# Add a dense layer with 64 units
model.add(layers.Dense(500, activation='relu'))

# Add a dense layer with 64 units
model.add(layers.Dense(250, activation='relu'))

# Output layer with Categorical Values
model.add(layers.Dense(10, activation='softmax'))  # Assuming 10 classes for multiclass classification

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print the model summary
model.summary()

# Train the model
model.fit(x_train, y_train, epochs=20, validation_split=0.3)

"""Test Model with Test Data"""

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

# Make predictions on the test data
y_pred = model.predict(x_test)

print(tf.argmax(y_pred[0]))

y_test_new = np.ones((x_test.shape[0],1))
y_pred_new = np.ones((x_test.shape[0],1))

for i in range(x_test.shape[0]):
  y_test_new[i] = np.array(tf.argmax(y_test[i]))
  y_pred_new[i] = np.array(tf.argmax(y_pred[i]))

print(y_test_new[1])
print(y_pred_new[1])

# Calculate the accuracy of the classifier
accuracy = accuracy_score(y_test_new, y_pred_new)

# Print the accuracy
print(accuracy)

# Create a confusion matrix
conf_matrix = confusion_matrix(y_test_new, y_pred_new)

plt.figure(figsize=(6, 6))
plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')

sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False,
            xticklabels=np.unique(y_test_new), yticklabels=np.unique(y_test_new))
plt.ylabel('True label')
plt.xlabel('Predicted label')

# Compute the F1 score
f1 = f1_score(y_test_new, y_pred_new, average='weighted')  # 'weighted' takes class imbalance into account

print("F1 Score:", f1)

"""Save Model as Wine Quality 1D-CNN"""

import pickle
# save pipeline
model.save('white_wine_pipeline_1DCNN')
# Saving model to disk
pickle.dump(model, open('white_wine_pipeline_1DCNN.pkl','wb'))

import pickle

# Saving model to disk
pickle.dump(final_best, open('white_wine_pipeline_pycaret2.pkl','wb'))

# Save the model's weights to a file
model.save_weights("white_wine_pipeline_1DCNN_weights.h5")